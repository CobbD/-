<nav id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgheadline5">1. 前言</a>
<ul>
<li><a href="#orgheadline1">1.1. 理解协程(coroutine)</a></li>
<li><a href="#orgheadline2">1.2. 理解 yield from</a></li>
<li><a href="#orgheadline3">1.3. loop-with-callbacks</a></li>
<li><a href="#orgheadline4">1.4. loop-with-coroutines</a></li>
</ul>
</li>
<li><a href="#orgheadline16">2. asyncio模块简介</a>
<ul>
<li><a href="#orgheadline10">2.1. 添加回调</a>
<ul>
<li><a href="#orgheadline6">2.1.1. add_reader方法</a></li>
<li><a href="#orgheadline7">2.1.2. remove_reader方法</a></li>
<li><a href="#orgheadline8">2.1.3. add_writer方法</a></li>
<li><a href="#orgheadline9">2.1.4. remove_writer方法</a></li>
</ul>
</li>
<li><a href="#orgheadline11">2.2. 自定义协议</a></li>
<li><a href="#orgheadline12">2.3. 添加Task</a></li>
<li><a href="#orgheadline15">2.4. 协程任务结果聚合</a>
<ul>
<li><a href="#orgheadline13">2.4.1. 使用Queue</a></li>
<li><a href="#orgheadline14">2.4.2. 使用gather函数</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline17">3. 参考资料</a></li>
</ul>
</div>
</nav>


# 前言<a id="orgheadline5"></a>

本文紧跟着 [套接字编程入门](套接字编程入门.html)  一文，继续向前探索。在本文开始前，强烈建议读者读读 [参考资料1](#orgtarget1) ，该文其中说明asyncio模块部分很是重要。

## 理解协程(coroutine)<a id="orgheadline1"></a>

读者请参看 David Beazley [这篇文章](http://www.dabeaz.com/coroutines/Coroutines.pdf) 。

我们看到这个例子:

```python
import time
def follow(thefile):
    thefile.seek(0,2)
    while True:
        line = thefile.readline()
        if not line:
            time.sleep(0.1)
            continue

        yield line


logfile = open("test.log")
for line in follow(logfile):
    print(line,end='')
```

然后我们执行:

    bash>>> echo "hello" >> test.log

然后我们开始运行上面的那个python小脚本。

然后我们再执行:

    bash>>> echo "hello" >> test.log

Interesting,这整个过程是怎样的呢。关于生成器函数最基本的东西这里就不罗嗦了，这里要讲的是，python中的每一个函数经过编译之后送入内存，并没有实际执行，而具体调用的时候如果遇到 `yield` ，python就会中止该函数的执行，去做其他的事情去了。而这个所谓的生成器函数返回的是一个生成器对象。我们看到 `follow(logfile)` 那里其返回的就是一个生成器对象。生成器对象有一个 `send` 方法，如果 send 的是 `None` ，那么该生成器就开始执行，并返回一个值，然后又遇到一个 `yield` 了，生成器又会停止，如此继续，直到返回 `StopIteration` 异常，然后我们就直到这个生成器迭代完了。

一般我们编程都尽量避免 `while True` 这样的写法的，但在生成器函数里面，却可以这样做，这样就避免了 `StopIteration` 异常，然后生成器就好像一个管道 流 监听器一样的东西了。如果说上面这个例子还不太难理解，那么看下面这个例子。

```python
def grep(pattern):
    print("Searching for", pattern)
    while True:
        line = (yield)
        if line is None:
            print('you send None')
        elif pattern in line:
            print(line)

search = grep('coroutine')
```

search是这么一个生成器对象，我们可以把生成器函数看作一个管道一样的东西，如果说前面的 `yield what` 是管道出口，那么这里的 `(yield)` 这样的表达就是管道入口。我们send过来的什么东西都将成为返回值。前面我们说 `send(None)` 生成器开始执行，实际上send(what)的what都是这里的 `(yield)` 的返回值。这样程序 

现在我们运行上面python脚本然后继续输入:

    >>> search.send(None)
    Searching for coroutine
    >>> search.send(None)
    you send None
    >>> search.send("test")
    >>> search.send("test coroutine")
    test coroutine
    >>>

我们看到第一次send None 生成器函数开始实际执行，然后遇到yield 停顿了。 然后 send None ，这时 line 的值就是 None ，然后等等等等，这个我们建立了一个生成器对象，而这个生成器对象好像一个管道一个长期挂载在那里，可以送信息，也可以输出信息，实在奇妙啊。而所谓的协程就是这种编程风格，基于这里讨论的生成器函数的作用原理，然后我们通过

    for i in generator:
        print(i)

以一种流的风格，慢慢送过来，再慢慢输出。这一块内容很丰富很艰深，我们后面有时间再仅需深入慢慢体会吧。

## 理解 yield from<a id="orgheadline2"></a>

在开始前有必要详细说明一下yield from关键词。我们从 [参考资料1](#orgtarget1) 看到python的内在处理机制是将所谓的协程概念和异步编程概念等都和python中的生成器迭代器概念重合了。实际上我们可以简单地将一个协程看作一个生成器对象，然后Task这个对象是基于Future这个对象的:

    class Task(Future):
        """A coroutine wrapped in a Future."""

而Future对象的理解关键点还是在生成器概念和yield from这个关键词上（请读者参看 [这个网页](http://simeonvisser.com/posts/python-3-using-yield-from-in-generators-part-1.html) ）。当然就作为python中函数的执行和生成器的惰性运算机制，比如:

    gen.send('None')

之后然后返回一个结果这些这里就不多说了。

我们先看到这个例子:

    def generator2():
        for i in range(10):
            yield i
    
    def generator3():
        for j in range(10, 20):
            yield j
    
    def generator():
        x= yield from generator2()
        print('return value of yield-from: {}'.format(x))
        yield from generator3()
    
    gen = generator()
    
    print(list(gen))

其结果输出如下:

    return value of yield-from: None
    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]

首先强烈建议读者试着将上面的 `yield from` 改成 `yield` 来试一下。这样我们看到yield from实现了一种生成器的 **组合** 逻辑。而直接yield某个生成器，最后得到的只是一个生成生成器的生成器罢了。

我们看到generator实现了这样的数据组合:

![img](images/协程数据组合.png "协程数据组合")

b和c这两个子协程或者说子生成器可以通过在一个新的函数中构建出一个新的协程函数，其数据是前面两个子协程数据的组合。

然后我们看到上面的 yield from 实际上并没有返回值，x的值是None。实际上python里面的函数yield语句和return语句并不互斥，看下面的例子:

```python
def gen_fn():
    result = yield 1
    print('result of yield: {}'.format(result))
    result2 = yield 2
    print('result of 2nd yield: {}'.format(result2))
    return 'done'

def caller_fn():
    gen = gen_fn()
    rv = yield from gen
    print('return value of yield-from: {}'.format(rv))


gen = caller_fn()
print(list(gen))
```

其输出如下:

    result of yield: None
    result of 2nd yield: None
    return value of yield-from: done
    [1, 2]

注意看 `gen_fn` 函数有return语句，那么这个生成器函数的生成对象通过yield from语句是可以获得返回值的，这个返回值由return语句。

最后来说明一下yield from语句和可迭代对象之间的关系:

    class Test():
        def __iter__(self):
            yield self
            return True
    
    def test():
        test = Test()
        x = yield from test
        print(x)
        return x
    
    gen = test()
    print(list(gen))

    True
    [<__main__.Test object at 0x7f66f7810ef0>]
    >>>

如果一个对象定义了 `__iter__` 方法，那么我们知道这个对象在python中称之为可迭代对象了，其可以通过 `for i in what` 这样的语句来迭代等。而yield from语句实际上的求值就是对某个python可迭代对象具体进行迭代得到的。这里x的值实际上是True，而gen的值是由函数yield from语句组合出来的值，其实际上对应的是该可迭代对象的 `__iter__` 方法，也就是yield self。

## loop-with-callbacks<a id="orgheadline3"></a>

通过DefaultSelector构建起来的事件循环还有基于套接字的操作，下面代码来自 [500lines](https://github.com/aosabook/500lines) 。若读者已经读过前面的套接字编程入门一文，那么理解这个脚本应该不成问题了。其中值得注意的是其刷url和处理重复url的python方法较好，值得我们学习。

```python
#!/usr/bin/env python3.4

"""Sloppy little crawler, demonstrates a hand-made event loop and callbacks."""

from selectors import *
import socket
import re
import urllib.parse
import time


urls_todo = set(['/'])
seen_urls = set(['/'])
concurrency_achieved = 0
selector = DefaultSelector()
stopped = False


class Fetcher:
    def __init__(self, url):
        self.response = b''
        self.url = url
        self.sock = None

    def fetch(self):
        global concurrency_achieved
        concurrency_achieved = max(concurrency_achieved, len(urls_todo))

        self.sock = socket.socket()
        self.sock.setblocking(False)
        try:
            self.sock.connect(('blog.cdwanze.org', 80))
        except BlockingIOError:
            pass
        selector.register(self.sock.fileno(), EVENT_WRITE, self.connected)

    def connected(self, key, mask):
        selector.unregister(key.fd)
        get = 'GET {} HTTP/1.0\r\nHost: blog.cdwanze.org\r\n\r\n'.format(self.url)
        self.sock.send(get.encode('utf-8'))
        selector.register(key.fd, EVENT_READ, self.read_response)

    def read_response(self, key, mask):
        global stopped

        chunk = self.sock.recv(4096)  # 4k chunk size.
        if chunk:
            self.response += chunk
        else:
            selector.unregister(key.fd)  # Done reading.
            links = self.parse_links()
            for link in links.difference(seen_urls):
                urls_todo.add(link)
                Fetcher(link).fetch()

            seen_urls.update(links)
            urls_todo.remove(self.url)
            if not urls_todo:
                stopped = True
            print(self.url)

    def body(self):
        body = self.response.split(b'\r\n\r\n', 1)[1]
        return body.decode('utf-8')

    def parse_links(self):
        if not self.response:
            print('error: {}'.format(self.url))
            return set()
        if not self._is_html():
            return set()
        urls = set(re.findall(r'''(?i)href=["']?([^\s"'&lt;&gt;]+)''',
                              self.body()))

        links = set()
        for url in urls:
            normalized = urllib.parse.urljoin(self.url, url)
            parts = urllib.parse.urlparse(normalized)
            if parts.scheme not in ('', 'http', 'https'):
                continue
            host, port = urllib.parse.splitport(parts.netloc)
            if host and host.lower() not in ('blog.cdwanze.org'):
                continue
            defragmented, frag = urllib.parse.urldefrag(parts.path)
            links.add(defragmented)

        return links

    def _is_html(self):
        head, body = self.response.split(b'\r\n\r\n', 1)
        headers = dict(h.split(': ') for h in head.decode().split('\r\n')[1:])
        return headers.get('Content-Type', '').startswith('text/html')


start = time.time()
fetcher = Fetcher('/')
fetcher.fetch()

while not stopped:
    events = selector.select()
    for event_key, event_mask in events:
        callback = event_key.data
        callback(event_key, event_mask)

print('{} URLs fetched in {:.1f} seconds, achieved concurrency = {}'.format(
    len(seen_urls), time.time() - start, concurrency_achieved))
```

## loop-with-coroutines<a id="orgheadline4"></a>

如果读者理解了yield from关键词，那么下面的代码也是很好理解的。这段代码基于上面的代码然后稍作修改而来。

```python
#!/usr/bin/env python3.4

"""Sloppy little crawler, demonstrates a hand-made event loop and coroutines.

First read loop-with-callbacks.py. This example builds on that one, replacing
callbacks with generators.
"""

from selectors import *
import socket
import re
import urllib.parse
import time


class Future:
    def __init__(self):
        self.result = None
        self._callbacks = []

    def result(self):
        return self.result

    def add_done_callback(self, fn):
        self._callbacks.append(fn)

    def set_result(self, result):
        self.result = result
        for fn in self._callbacks:
            fn(self)

    def __iter__(self):
        yield self  # This tells Task to wait for completion.
        return self.result


class Task:
    def __init__(self, coro):
        self.coro = coro
        f = Future()
        f.set_result(None)
        self.step(f)

    def step(self, future):
        try:
            next_future = self.coro.send(future.result)
        except StopIteration:
            return

        next_future.add_done_callback(self.step)


urls_seen = set(['/'])
urls_todo = set(['/'])
concurrency_achieved = 0
selector = DefaultSelector()
stopped = False


def connect(sock, address):
    f = Future()
    sock.setblocking(False)
    try:
        sock.connect(address)
    except BlockingIOError:
        pass

    def on_connected():
        f.set_result(None)

    selector.register(sock.fileno(), EVENT_WRITE, on_connected)
    yield from f
    selector.unregister(sock.fileno())


def read(sock):
    f = Future()

    def on_readable():
        f.set_result(sock.recv(4096))  # Read 4k at a time.

    selector.register(sock.fileno(), EVENT_READ, on_readable)
    chunk = yield from f
    selector.unregister(sock.fileno())
    return chunk


def read_all(sock):
    response = []
    chunk = yield from read(sock)
    while chunk:
        response.append(chunk)
        chunk = yield from read(sock)

    return b''.join(response)


class Fetcher:
    def __init__(self, url):
        self.response = b''
        self.url = url

    def fetch(self):
        global concurrency_achieved, stopped
        concurrency_achieved = max(concurrency_achieved, len(urls_todo))

        sock = socket.socket()
        yield from connect(sock, ('blog.cdwanze.org', 80))
        get = 'GET {} HTTP/1.0\r\nHost: blog.cdwanze.org\r\n\r\n'.format(self.url)
        sock.send(get.encode('utf-8'))
        self.response = yield from read_all(sock)

        self._process_response()
        urls_todo.remove(self.url)
        if not urls_todo:
            stopped = True
        print(self.url)

    def body(self):
        body = self.response.split(b'\r\n\r\n', 1)[1]
        return body.decode('utf-8')

    def _process_response(self):
        if not self.response:
            print('error: {}'.format(self.url))
            return
        if not self._is_html():
            return
        urls = set(re.findall(r'''(?i)href=["']?([^\s"'&lt;&gt;]+)''',
                              self.body()))

        for url in urls:
            normalized = urllib.parse.urljoin(self.url, url)
            parts = urllib.parse.urlparse(normalized)
            if parts.scheme not in ('', 'http', 'https'):
                continue
            host, port = urllib.parse.splitport(parts.netloc)
            if host and host.lower() not in ('blog.cdwanze.org'):
                continue
            defragmented, frag = urllib.parse.urldefrag(parts.path)
            if defragmented not in urls_seen:
                urls_todo.add(defragmented)
                urls_seen.add(defragmented)
                Task(Fetcher(defragmented).fetch())

    def _is_html(self):
        head, body = self.response.split(b'\r\n\r\n', 1)
        headers = dict(h.split(': ') for h in head.decode().split('\r\n')[1:])
        return headers.get('Content-Type', '').startswith('text/html')


start = time.time()
fetcher = Fetcher('/')
Task(fetcher.fetch())

while not stopped:
    events = selector.select()
    for event_key, event_mask in events:
        callback = event_key.data
        callback()

print('{} URLs fetched in {:.1f} seconds, achieved concurrency = {}'.format(
    len(urls_seen), time.time() - start, concurrency_achieved))
```

# asyncio模块简介<a id="orgheadline16"></a>

一般首先你需要通过 `get_event_loop` 函数来获取一个全局性的事件驱动循环，其返回一个EventLoop对象，asyncio模块为EventLoop对象提供了很多方法，很多任务都可以通过调用这个EventLoop对象的方法来完成，下面简称为loop。

`run_until_complete` 方法是本来是要接受一个Future对象，然后将其执行完。如果接受的是一个协程对象（coroutine object），则要将其转变成为Task对象（Task对象是Future对象的子类）。

loop的 `close` 方法，关闭事件循环。loop的 `stop` 方法停止运行事件循环，和close方法的区别就是stop方法之前回调的函数还会继续运行，之后的不会（如果后面又有 run\_forever 语句，则后面回调的那些函数又会被执行。）。而close方法是完全强制中止了。 然后loop的 `run_forever` 方法是永久运行事件循环，直到stop方法被调用。

上面这些前面也谈过一些了，都是最基本的知识。

## 添加回调<a id="orgheadline10"></a>

### add\_reader方法<a id="orgheadline6"></a>

    BaseEventLoop.add_reader(fd, callback, *args)

事件循环对象的 add\_reader 方法，监听某个文件，如果可读事件发生，则执行callback函数，后面是传递给callback函数的一些参数。

### remove\_reader方法<a id="orgheadline7"></a>

移除某个reader。

### add\_writer方法<a id="orgheadline8"></a>

监听可写事件。

    BaseEventLoop.add_writer(fd, callback, *args)

### remove\_writer方法<a id="orgheadline9"></a>

移除某个writer。

## 自定义协议<a id="orgheadline11"></a>

自定义的协议继承自Protocol类，然后其调用loop的 `create_server` 来接受这个协议类来时间创建一个协程式的服务器程序:

    coroutine BaseEventLoop.create_server(protocol_factory, host=None, port=None, ...)

大概如下这个例子所示:

```python
import asyncio

loop = asyncio.get_event_loop()

class EchoProtocol(asyncio.Protocol):
    def connection_made(self, transport):
        self.transport = transport

    def data_received(self, data):
        self.transport.write(data)

    def connection_lost(self, exc):
        server.close()

server = loop.run_until_complete(loop.create_server(EchoProtocol, '127.0.0.1', 4444))
loop.run_until_complete(server.wait_closed())
```

上面的代码运行效果大致如下所示:

    wanze@wanze-ubuntu:~$ netcat localhost 4444
    d
    d
    a
    a
    ^C

就是通过netcat你输入什么服务器那边就返回什么。

其定义的方法有:

-   **connection\_made:** 

这个callback继承自Protocol类，逻辑是如果一个连接建好了，那么执行该函数。其接受一个参数transport。也就是具体协议的传输层。

-   **data\_received:** 

这个callback继承自Protocol类，如果某个数据传进来了，那么该函数将被执行。其接受一个参数就是传进来的data。

-   **eof\_received:** 

数据结束完毕是调用。你可以在另外一端用transport发送写入结束信号 `write_eof()` 。

## 添加Task<a id="orgheadline12"></a>

通过loop的 `create_task` 方法来给事件循环添加一个Task任务。一般我们使用不用再去考虑Future的概念了，简单理解就是给loop事件循环添加一个待执行的协程任务，然后这个任务对象可以内部可以添加计划多个任务流，大抵如此。

    BaseEventLoop.create_task(coro)

## 协程任务结果聚合<a id="orgheadline15"></a>

### 使用Queue<a id="orgheadline13"></a>

    from asyncio import JoinableQueue as Queue

asyncio提供了Queue对象支持。一般使用将其绑定在主事件循环上。

    queue = Queue(loop=self.loop)

然后其含的重要方法有:

-   **get方法:** 获取一个item，协程方法。（你需要先确保task\_done，之后item才能正常使用，否则你get的是一个协程对象）
-   **put方法:** 放入一个任务协程对象，如果queue满了则会等待，协程方法。
-   **task\_done方法:** 阻塞程序，确保queue下一次要get的item已经完成了（也就是协程函数已经展开了）
-   **put\_nowait方法:** 放入一个任务协程对象不阻塞，如果queue满了则会抛出异常（若Queue设的是默认的maxsize=0，则queue永远都不会满的）
-   **join方法:** queue里所有的item任务都要完成，程序有个计数器，若加入一个任务到queue，则计数器加一，若一个task\_done()被执行，则计数器减去一，如果计数器未完成任务等于0了，则join方法unblock。但需要注意的是这个join方法本身也是协程式的，即其对于主程序来说本没有阻塞。

### 使用gather函数<a id="orgheadline14"></a>

    asyncio.gather(*coros_or_futures, loop=None, return_exceptions=False)

gather函数将收集一些协程函数或任务或futures等，等所有的结果都聚合之后，将返回一个所含结果的列表（以你原先指定的各个协程的顺序）。

`return_excepitons` 默认是False，也就是其内收集的这些协程如果有一个发生异常了，那么将视为整体发生异常。如果设为True，则允许个别子协程发生异常，而且这些异常被视为结果放入列表中。

然后 `CancelledError` 异常，如果是上层Future抛出的cancel信号，则其内所有的子任务都将被cancel，而如果某个子任务被cancel，则会抛出 `CancelledError` ，不影响其他子任务。

# 参考资料<a id="orgheadline17"></a>

1.  a-web-crawler-with-asyncio-coroutines [这种原英文网页](http://aosabook.org/en/500L/a-web-crawler-with-asyncio-coroutines.html) , [这里有个中文翻译网页](http://damnever.github.io/2015/10/12/a-web-crawler-with-asyncio-coroutines/) 。 <a id="orgtarget1"></a>
2.  [playing-with-asyncio](http://www.getoffmalawn.com/blog/playing-with-asyncio)